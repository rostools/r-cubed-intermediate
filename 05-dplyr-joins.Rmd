# Workflow to analyzing your tidy data {#dplyr-2}

```{r setup-05, include=FALSE, eval=FALSE}
source("R/functions.R")
library(here)
library(fs)
library(tidyverse)
library(vroom)
source_session("03-functions.Rmd")
source_session("04-functionals.Rmd")
load("data/mmash.rda")
```

Here we will continue using the *Workflow* block as we cover the fourth block,
"*Work with final data*" in Figure \@ref(fig:diagram-overview-4).

```{r diagram-overview-4, fig.cap="Section of the overall workflow we will be covering.", echo=FALSE}
diagram_overview(4)
```

And your folder and file structure should look like:

```
LearnR3
├── data/
│   ├── mmash.rda
│   └── README.md
├── data-raw/
│   ├── mmash-data.zip
│   ├── mmash/
│   │  ├── user_1
│   │  ├── ...
│   │  └── user_22
│   └── mmash.R
├── doc/
│   ├── README.md
│   └── lesson.Rmd
├── R/
│   ├── functions.R
│   └── README.md
├── .Rbuildignore
├── .gitignore
├── DESCRIPTION
├── LearnR3.Rproj
└── README.md
```

## Learning objectives

## Processing character data

**Take 8 min to read this section before we quickly go over it together**.
When processing data, you likely will encounter and deal with cleaning up
character data. A wonderful package to use for working with character data is
called [stringr]. We'll use that in order to process the `file_path_id` 
so that we can get the user ID from it. 

The main driver behind the functions in stringr are [regular expressions] (or
[regex] for short). These expressions are powerful, very concise ways of finding
patterns in text. We've already used them in the `dir_ls()` function with the
`regexp` argument to find our data files.

To give an example, the regex `^.*[1-9][0-9]?` means "starting from the
beginning of the text, find any characters one or more times and stop at a
number from 0 to 9, include a possible other number from 0 to 9". So, using this
on a string like `"hi there, it's 30 degrees out"`, the regex would select `"hi
there, it's 30"`. If we break it down:

- `^` means start of the string.
- `.` means match any character once.
- `*` means match the previous character one or more times.
- `[]` means match one time for each individual character within the bracket 
(e.g. `[user]` doesn't search for the word `user` it searches for characters,
`u`, `s`, `e`, `r` individually, *but not* all together).
- `0-9` means match any number from 0 to 9.
- `?` means the previous item may or may not be matched.

Confused? Yea, regex does that to pretty much everyone, you aren't alone. While
regex can be very very powerful, it can also be incredibly difficult to write
and work with. We won't cover this anymore in this course, but two great resources
are the [R for Data Science regex section](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions),
the [stringr regex page](https://stringr.tidyverse.org/articles/regular-expressions.html),
as well as in the help doc `?regex`.
For now, we will only use it for the simplest purposes possible:
to extract `user_1` to `user_22` from the `file_path_id`.
**Please stop here and we'll go over this together**.

[stringr]: https://stringr.tidyverse.org/

`r details_for_instructors("
Make sure to re-inforce that while regex is incredibly complicated, there are some
basic things you can do with it that are quite powerful.

More or less, this section is to introduce the idea and concept of regex, but not
to really teach it since that is well beyond the scope of this course and this
time frame.
")`

## Exercise: Brainstorm a regex that will match for the user ID

Time: 5 min

Discuss with your neighbour what a potential regex might be to find
the ID in the `file_path_id`. Try not to look ahead `r emo::ji("wink")` since we
will use this regex later on. When the time is up, we'll share possible ideas.

```{r solution-regex-brainstorm, echo=FALSE}
# There is no code for this exercise.
```

## Working with character columns

Now that we've talked about regex and pipes, let's start using them.
The first thing we'll do is work with the `user_info_df` to write
code that works, after which we will convert it into a function and move it
into the `R/functions.R` file.

We want to create a new column for the user ID, so we will use the `mutate()`
function from the [dplyr] package. We'll use the regex `user_[0-9][0-9]?` to match
for the user ID and we'll use the `str_extract()` function from the stringr
package. So, in your `doc/lesson.Rmd` file, create a new header called 
`## Using regex for user ID` at the bottom of the document, and create a new
code chunk below that.

[dplyr]: https://dplyr.tidyverse.org/

`r details_for_instructors("
Walk through writing this code, explain about how to use mutate, and about
the stringr function.
")`

```{r extract-user-id, eval=FALSE}
# Note: your file paths and data may look slightly different.
user_info_df %>% 
    mutate(user_id = str_extract(file_path_id, "user_[0-9][0-9]?"))
```

```{r extract-user-id-eval, echo=FALSE}
user_info_df %>% 
    mutate(file_path_id = gsub(".*\\/data-raw", "data-raw", file_path_id)) %>% 
    mutate(user_id = str_extract(file_path_id, "user_[0-9][0-9]?")) %>% 
    relocate(file_path_id, user_id)
```

Since we don't need to keep the `file_path_id`, let's drop it using `select()`
and `-`.

```{r drop-file-path-id}
user_info_df %>% 
    mutate(user_id = str_extract(file_path_id, "user_[0-9][0-9]?")) %>% 
    select(-file_path_id)
```

## Exercise: Convert this code into a function

Time: 10 min

We now have code that takes the data that has the `file_path_id` column
and extracts the user ID from it. While in the `doc/lesson.Rmd` file, convert
this code into a function, using the same process you've done previously.

- Call the new function `extract_user_id` and add one argument called
`imported_data`.
    - Don't forget to output the code into an object and adding `return()` at
    the end with the object inside it. 
    - Also include Roxygen documentation.
- After writing it and testing it, move the function into `R/functions.R`.
- Replace the code in the `doc/lesson.Rmd` file with the function name (e.g.
`extract_user_id(user_info_df)`), restart the R session, source everything with
`source()` (`Ctrl-Shift-S`), and run the new function.

*Tip*: If you don't know what package a function comes from when you
need to append the package when using `::`, you can find out what the package is
by using the help documentation `?functionname` (can also be done by pressing F1
when the cursor is over the function). The package name is at the very top left
corner, surrounded by `{ }`.

```{r solution-user-id-extract, echo=FALSE}
extract_user_id <- function(imported_data) {
    extracted_id <- imported_data %>% 
        dplyr::mutate(user_id = stringr::str_extract(file_path_id, 
                                                     "user_[0-9][0-9]?")) %>% 
        dplyr::select(-file_path_id)
    return(extracted_id)
}
```

Since we want this function to work on all data that we import, we should add it
to `import_multiple_files()`. After you've created the function, go to the
`import_multiple_files()` function in `R/functions.R` and use the `%>%` to
add it after using the `map_dfr()` function. The code should look something like:

```{r add-extract-user-to-import}
import_multiple_files <- function(file_pattern, import_function) {
    data_files <- fs::dir_ls(here::here("data-raw/mmash/"),
                             regexp = file_pattern,
                             recurse = TRUE)
    
    combined_data <- purrr::map_dfr(data_files, import_function,
                                    .id = "file_path_id") %>% 
        extract_user_id()
    return(combined_data)
}
```

Re-source the functions with `source()` (`Ctrl-Shift-S`). Then re-run these
pieces of code you wrote during Exercise \@ref(ex-function-import-all-data) to
update them based on the new code in the `import_multiple_files()` function.
Something like this should already be in somewhere in your `doc/lesson.Rmd` file.

```{r}
user_info_df <- import_multiple_files("user_info.csv", import_user_info)
saliva_df <- import_multiple_files("saliva.csv", import_saliva)
rr_df <- import_multiple_files("RR.csv", import_rr)
actigraph_df <- import_multiple_files("Actigraph.csv", import_actigraph)
```

Lastly, **don't forget** to add and commit the files into the Git history.

## Join datasets together

`r details_for_instructors("
Walk through and describe these images and the different type of joins.
")`

The ability to join datasets together is a fundamental component of data processing
and transformation. In our case, we want to add the datasets together so we 
eventually have at least one main dataset to work with.

There are many ways to join datasets, the more common ones that are implemented
in the dplyr package are:

- `left_join(x, y)`: Join all rows and columns in `y` that match rows and
columns in `x`. *Columns* that exist in `y` but not `x` are joined to `x`.

    ```{r image-left-join, fig.cap="Left joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/left-join.png"))
    ```
    
- `right_join(x, y)`: The opposite of `left_join()`. Join all rows and columns
in `x` that match rows and columns in `y`. *Columns* that exist in `x` but not `y`
are joined to `y`.

    ```{r image-right-join, fig.cap="Right joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/right-join.png"))
    ```

- `full_join(x, y)`: Join all rows and columns in `y` that match rows and
columns in `x`. Columns *and* **rows** that exist in `y` but not `x` are joined
to `x`. 

    ```{r image-full-join, fig.cap="Full joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/full-join.png"))
    ```

[dplyr-cheatsheet]: https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-transformation.pdf

In our case, we want to use `full_join()`, since we want all the data from both
datasets. This function takes two datasets and lets you indicate which column
to join by using the `by` argument. Here, both datasets have the column `user_id`
so we will join by them.

```{r}
full_join(user_info_df, saliva_df, by = "user_id")
```

`full_join()` is useful if we want to include all values from both datasets,
as long as each participant ("user") had data collected from that dataset.
When the two datasets have rows that don't match, we will get missingness in that
row, but that's ok in this case.

We also eventually have other datasets to join together later on. Since
`full_join()` can only take two datasets at a time, do we then just keep 
using `full_join()` until all the other datasets are combined?
What if we get more data later on? Well, that's where more functional programming
comes in. Again, we have a simple goal: For a set of data frames, join them
all together. Here we use another functional programming concept called `reduce()`.
Like `map()`, which "maps" a function onto a set of items, `reduce()`
applies a function to each item of a vector or list, each time reducing the set
of items down until only one remains: the output. Let's use our simple function
`add_numbers()` from before and add up 1 to 5. Since `add_numbers()` only takes
two numbers, we have to give it two numbers at a time and repeat until we reach
5.

```{r}
# Add from 1 to 5
first <- add_numbers(1, 2)
second <- add_numbers(first, 3)
third <- add_numbers(second, 4)
add_numbers(third, 5)
```

Instead, we can use reduce to do the same thing:

```{r}
reduce(1:5, add_numbers)
```

Figure \@ref(fig:image-reduce) visually shows what is happening within `reduce()`.

```{r image-reduce, fig.cap="A functional that iteratively uses a function on a set of items until only one output remains. Modified from the [RStudio purrr cheatsheet][purrr-cheatsheet].", out.width="75%", echo=FALSE}
knitr::include_graphics(here::here("images/reduce.png"))
```

[purrr-cheatsheet]: https://raw.githubusercontent.com/rstudio/cheatsheets/master/purrr.pdf

Since `reduce()`, like `map()`, takes either a vector or a list as an input, and
since data frames can only be put together as a list (a data frame has vectors
for columns and so can't be a vector itself), we need to combine the datasets
together in a `list()` and reduce them with `full_join()`:

```{r}
combined_data <- reduce(list(user_info_df, saliva_df), full_join)
combined_data
```


We now have the data in a form that would make sense to join it with the other
datasets. So lets try it:

```{r}
reduce(list(user_info_df, saliva_df, summarised_rr_df), full_join)
```

Hmm, but wait, we now have four rows of each user, when really we should have 
only two, one for each day. That's because `saliva_df` doesn't have a `day` column,
instead there is a `samples` column. We'll need to add a day column in order
to join properly with the RR dataset. 

There are several ways to do this, but probably the easiest, most explicit, and
programmatically accurate way of doing it would be with the function
`case_when()`. This function works by providing it with a series of logic
conditions and an associated output if the condition is true. The general form
looks like:

```r
case_when(
    variable1 == condition1 ~ output,
    variable2 == condition2 ~ output,
    # (Optional) Otherwise
    TRUE ~ final_output
)
```

The optional ending is only necessary if you want a certain output if none of your
conditions are met. By default, the final output would be a missing value.
A (silly) example using age might be:

```r
case_when(
    age > 20 ~ "old",
    age <= 20 ~ "young",
    # For final condition
    TRUE ~ "unborn!"
)
```

If instead you want one of the conditions to be `NA`, you need to set the appropriate
`NA` value:

```r
case_when(
    age > 20 ~ "old",
    age <= 20 ~ NA_character_,
    # For final condition
    TRUE ~ "unborn!"
)
```

With dplyr functions like `case_when()`,
it requires you be explicit about the type of output each condition has since 
all the outputs must match (e.g. all character or all numeric).
This prevents you from accidentally mixing e.g. numeric output with character
output. This includes for missing values. Other explicit `NA` include:

- `NA_real_` (numeric)
- `NA_integer_` (integer)

Assuming the final output is `NA`, in a pipeline this would look like how you
normally would use `mutate()`:

```{r}
user_info_df %>% 
    mutate(age_category = case_when(
        age > 20 ~ "old",
        age <= 20 ~ "young"
    ))
```

**Ok, please stop reading and we will code together again**.

By using this function, we can set `"before sleep"` as day 1 and `"wake up"` as
day 2 by creating a new column called `day` that uses the `case_when()` function.
(We will use `NA_real_` because the other `day` columns are numeric, not integer.)

```{r}
saliva_with_day_df <- saliva_df %>% 
    mutate(day = case_when(
        samples == "before sleep" ~ 1,
        samples == "wake up" ~ 2
    ))
saliva_with_day_df
```

## Summarizing data prior to joining

TODO: Update this section
...Now, let's use the `reduce()` with `full_join()` again:

```{r}
reduce(list(user_info_df, saliva_with_day_df, summarised_rr_df), full_join)
```

We now have two rows per participant!

## Exercise: Join the summarized Actigraph data

Time: 20 min

TODO: Update this exercise
Like with the `RR.csv` dataset, let's process the `Actigraph.csv` dataset so 
that you can join it with the other datasets.

1. Like usual, create a new Markdown header called e.g. `## Exercise: Summarise 
and join Actigraph` and insert a new code chunk below that.
2. Look into the [Data Description][mmash-site] to find out what each column 
is for.
3. Based on the documentation, which variables would you be most interested in
analyzing more?
    - Keep those columns as well `user_id` and `day` by using `select()`.
4. Decide which summary measure(s) you think may be most interesting for you
(e.g. `median()`, `sd()`, `mean()`, `max()`, `min()`, `var()`).
5. Using `group_by()` of `user_id` and `day`, summarize the variables with the
summary functions you chose.
6. Put this new data into an new object called e.g. `summarised_actigraph_df`
and include it with the other datasets in the `reduce()` function.
7. Add and commit the changes you've made into the Git history.

```{r solution-TODO, echo=FALSE, include=FALSE}
```

## Exercise: Import and process the activity data

Time: 25 minutes

We have a few other datasets that we could join together, but would likely require
more processing in order to appropriately join with the other datasets.
Complete these tasks:

1. Create a new header called `## Exercise: Importing activity data`
1. Create a new code chunk below this new header.
1. Starting the workflow from the beginning (e.g. with the `spec()` process),
write code that imports the `Activity.csv` data into R. 
1. Convert this code into a new function using the workflow you've used from
this course:
    - Call the new function `import_activity`.
    - Include one argument called `file_path`
    - Test that it works.
    - Add Roxygen documentation and explicit package links (`::`) with the
    functions.
    - Move the newly created function into the `R/functions.R`.
    - Use the new function in `doc/lesson.Rmd` and use `source()`
    (`Ctrl-Shift-S`) to run it.
1. Import all the `user_` datasets with `import_multiple_files()` and the
`import_activity()` function.
1. Pipe the results into `mutate()` and create a new column called
`activity_seconds` that is based on subtracting `end` and `start`.
    - Use `?mutate` and check the examples if you don't recall how to use this
    function.
1. Add and commit your changes to the Git history.

```{r solution-import-activity, echo=FALSE, include=FALSE}
import_activity <- function(file_path) {
    activity_data <- vroom::vroom(
        "data-raw/mmash/user_1/Activity.csv",
        col_select = -1,
        col_types = vroom::cols(
            Activity = vroom::col_double(),
            Start = vroom::col_time(format = ""),
            End = vroom::col_time(format = ""),
            Day = vroom::col_double()
        ),
        .name_repair = snakecase::to_snake_case
    ) 
    return(activity_data)
}

activity_df <- import_multiple_files("Activity.csv", import_activity)

activity_df %>% 
    # group_by(user_id) %>% 
    mutate(activity_duration = end - start)
```


Look into the [Data Description][mmash-site] and find out what each column
represents and what the numbers mean in the column `activity`. Then brainstorm
with your neighbour:

1. The disadvantage of using numbers instead of text to describe categorical
data like in the `activity` column.
1. Ways you could include this information into the dataset.
1. How you could meaningfully join this dataset with the other datasets.

Afterwards, we'll quickly discuss some of these ideas as a group.

[mmash-site]: https://physionet.org/content/mmash/1.0.0/

## Wrangling data into final form

Now that we've got several datasets processed and joined, its now time to 
bring it all together and put it into the `data-raw/mmash.R` script so we
can create a final working dataset.

Open up the `data-raw/mmash.R` file and let's start cutting and pasting
the code from `doc/lesson.Rmd` related to importing and joining the
`user_info.csv`, `saliva.csv`, `RR.csv`, and `Actigraph.csv`.

The script so far should look like:

```{r data-raw-mmash-script, eval=FALSE}
library(here)
mmash_link <- "https://physionet.org/static/published-projects/mmash/multilevel-monitoring-of-activity-and-sleep-in-healthy-people-1.0.0.zip"
download.file(mmash_link, destfile = here("data-raw/mmash-data.zip"))
unzip(here("data-raw/mmash-data.zip"), 
      exdir = here("data-raw"),
      junkpaths = TRUE)
unzip(here("data-raw/MMASH.zip"),
      exdir = here("data-raw"))

library(fs)
file_delete(here(c("data-raw/MMASH.zip", 
                   "data-raw/SHA256SUMS.txt",
                   "data-raw/LICENSE.txt")))
file_move(here("data-raw/DataPaper"), here("data-raw/mmash"))
```

First thing to do is comment out the `download.file()` code, since
we don't want to download it everytime we run this script. Next we'll want to do
rearrange it so all the packages are at the top, and then add the other
packages we'll need: tidyverse and vroom.

```{r}
library(tidyverse)
library(vroom)
library(fs)
library(here)
```

Next, we need to let this script know where to get our functions from. So right
below the `library()` functions, add:

```r
source(here("R/functions.R"))
```

This is the code that runs when you use `Ctrl-Shift-S`.

Now, find the `import_multiple_files()`, summarized RR and Actigraph data,
saliva data with the day column, and the `reduce()` code in your
`doc/lesson.Rmd` file and cut and paste it at the bottom of the
`data-raw/mmash.R` script. This is what the bottom half should look like:

```{r data-raw-mmash-2, eval=-c(1:17)}
library(tidyverse)
library(vroom)
library(fs)
library(here)
source(here("R/functions.R"))

mmash_link <- "https://physionet.org/static/published-projects/mmash/multilevel-monitoring-of-activity-and-sleep-in-healthy-people-1.0.0.zip"
# download.file(mmash_link, destfile = here("data-raw/mmash-data.zip"))
unzip(here("data-raw/mmash-data.zip"), 
      exdir = here("data-raw"),
      junkpaths = TRUE)
unzip(here("data-raw/MMASH.zip"),
      exdir = here("data-raw"))

file_delete(here(c("data-raw/MMASH.zip", 
                   "data-raw/SHA256SUMS.txt",
                   "data-raw/LICENSE.txt")))
file_move(here("data-raw/DataPaper"), here("data-raw/mmash"))

user_info_df <- import_multiple_files("user_info.csv", import_user_info)
saliva_df <- import_multiple_files("saliva.csv", import_saliva)
rr_df <- import_multiple_files("RR.csv", import_rr)
actigraph_df <- import_multiple_files("Actigraph.csv", import_actigraph)

summarised_rr_df <- rr_df %>% 
    group_by(user_id, day) %>% 
    summarise(across(ibi_s, list(mean = mean, sd = sd), na.rm = TRUE))

saliva_with_day_df <- saliva_df %>% 
    mutate(day = case_when(
        samples == "before sleep" ~ 1,
        samples == "wake up" ~ 2,
        TRUE ~ NA_real_
    ))

# Your Actigraph code will be probably be different
summarised_actigraph_df <- actigraph_df %>% 
    group_by(user_id, day) %>% 
    summarise(across(hr, list(mean = mean, sd = sd)))

mmash <- reduce(
    list(
        user_info_df,
        saliva_with_day_df,
        summarised_rr_df,
        summarised_actigraph_df
    ),
    full_join
)
```

Lastly, we have to save this final dataset into the `data/` folder. We'll use this
function `usethis::use_data()` to create the folder and save the data as an `.rda`
file. We'll add this code to the very bottom of the script:

```{r, eval=FALSE}
usethis::use_data(mmash, overwrite = TRUE)
```

We're adding `overwrite = TRUE` so every time we re-run this script, the dataset
will be saved. If the final dataset is going to be really large, we could save
it as a `.csv` file with:

```{r, eval=FALSE}
vroom_write(mmash, here("data/mmash.csv"))
```

And later load it in with `vroom()` (since it is so fast).
Alright, we're finished creating this dataset! Let's generate it
by first:

- Restart the R session with `Ctrl-Shift-F10` ("Session -> Restart R").
- Source the `data-raw/mmash.R` script with `Ctrl-Shift-S` ("Code -> Source" or
the "Source" button in the top right corner of the script pane).

TODO: Fix this section.
We now have a final dataset to start working on! The main way to load data is with
`load(here::here("data/mmash.rda"))`.

## Exercise: What other cleaned data might you create?

Time: 10 min

In order to meaningfully join all the datasets together, we have to calculate
summary measures for the `RR.csv` and `Actigraph.csv` data. However,
we also lose a lot of information and potentially interesting analyses from it.
With your neighbour, discuss some potential ways that you might join the datasets
to retain that information and what questions you might be interested in with
that data.

```{r solution-other-data-brainstorm, echo=FALSE}
# There is no code for this exercise.
```
