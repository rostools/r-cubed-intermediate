---
search: false
format: r3-theme-revealjs
bibliography: "../includes/references.bib"
csl: "../includes/vancouver.csl"
---

# Welcome to the Intermediate R^3^ workshop!

-   :heavy_check_mark: Pick a group from the basket and go to that table
-   :heavy_check_mark: Introduce yourself to your table mates
-   :heavy_check_mark: Open your `LearnR3.Rproj` RStudio project

::: notes
Introduce teachers and helpers.
:::

# Motivation for this workshop {.center}

## How do you *exactly* do data analysis? What's the workflow? :thinking: {.center}

::: notes
When I started out doing research during my Masters, I always wondered
*how* do researchers go about doing data analysis... what was their
workflow *exactly* like. No one ever really taught that. Even on online
tutorials, mostly pieces of code or how to use code are taught... but
never teaching the bigger picture... How do researchers in their daily
work write R code and do their data analysis?

This question is the reason why the overall workflow is the primary
focus in this workshop and partly with the beginner one. I try to focus
on the bigger picture and the overall workflow you would do for doing
data analysis.

So, why is it that there isn't much information on how researchers do
data analysis?
:::

# :shrug::confused:...Because code sharing is almost non-existent in science {.center}

Very few papers provide code [@Leek2017a, @Considine2017a]

::: notes
It's because code sharing basically doesn't exist in the vast majority
of scientific fields. You've probably read a methods section in a paper
and wondered how exactly they did it.
:::

## Code sharing: From scientific principle of "reproducibility" {.center}

...often confused with "replicability" [@Plesser2018a] (see also
American Statistical Association
[statement](https://www.amstat.org/asa/files/pdfs/POL-ReproducibleResearchRecommendations.pdf))

. . .

::::: columns
::: {.column width="50%"}
### Replicability

-   Repeating a study by *independently* performing another identical
    study
-   Linked to the "irreproducibility crisis" (aka "irreplicability
    crisis")
:::

::: {.column width="50%"}
### Reproducibility

-   Generating the exact same results when using the same data and code
-   *Question*: If we can't even *reproduce* a study's results, how can
    we expect to replicate it?
:::
:::::

## How can we check reproducibility if no code is given? {.center}

This is a little bit of a rhetorical question
:stuck_out_tongue_closed_eyes:

## Very low reproducibility in most of science [@Samuel2024] {.smaller}

::: {layout-ncol="2"}
```{r}
#| echo: false
#| fig-height: 6
library(tidyverse)
theme_set(
  theme_minimal() +
    theme(
      plot.background = element_rect(fill = "#E9E9E9"),
      axis.title = element_blank(),
      axis.text.y = element_text(size = 18),
      axis.text.x = element_blank(),
      axis.ticks = element_blank(),
      axis.line = element_blank()
    )
)

search <- tribble(
  ~item, ~description, ~value, ~order,
  "Articles", "Found among all articles", 3467, 1,
  "GitHub\nrepos", "Those with a GitHub link and at least one notebook", 2660, 2
) |>
  mutate(
    item = fct_rev(fct_reorder(item, order))
  )

ggplot(search, aes(y = item, x = value, label = value)) +
  geom_col(fill = "#203C6E") +
  geom_text(nudge_x = 200, size = 5.5)
```

```{r}
#| echo: false
#| fig-height: 6
repro_results <- tribble(
  ~item, ~description, ~value, ~order,
  "Total notebooks", "Total notebook files found in all the repositories", 27271, 1,
  "Notebook repo\nwas valid", "File names were correct, dependency information was available, no local modules were needed.", 15817, 2,
  "Could install\ndependencies", "Could install dependencies without errors.", 10388, 3,
  "Finished executing\nwithout errors", "Could run without any errors.", 1203, 4,
  "Same results\nas paper", "When the results of the paper matched what the execution results output.", 879, 5
) |>
  mutate(
    percent = str_c(value, "\n(", round(value / max(value) * 100), "%)"),
    item = fct_rev(fct_reorder(item, order))
  )

ggplot(repro_results, aes(y = item, x = value, label = percent)) +
  geom_col(fill = "#203C6E") +
  geom_text(nudge_x = 1700, size = 5.5)
```
:::

## Even in institutional code and data archive, *executability* is low! [@Trisovic2022] {.center}

-   Code taken from [Harvard Dataverse Project](https://dataverse.org/)
    data repositories
-   Only 25% could be *executed* without some "cleaning up"
-   After some automatic cleaning, \~50% could *execute*

::: notes
Recent large study on general reproducibility of projects that shared
code.

Initially only 25% of the R scripts could be *executed* (doesn't mean
results were reproduced though). After doing automatic and some manual
code cleaning, than about half could be executed. That's not bad.

Since scripts were taken from Dataverse.org, researchers who upload
their code and projects to it probably are a bit more aware and
knowledgeable about general reproducibility and coding then the average
researcher, so the results are a bit biased.
:::

## Scientific culture is not well-prepared for analytic and computation era {.center}

## These issues can be fixed by creating and nurturing a culture of openness {.center}

::: notes
All of this is because of a problem with our culture in research. We
aren't open, we don't really share, and don't often follow basic
principles of science. To fix this, we need to start creating and
nurturing a better and healthier culture. We all can be involved in
that, we all have that power to do something, even if its small thing.
:::

## Goal of this course: Start changing the culture by providing the training {.center}

# References
