# Importing data, fast! {#import-data}

![](https://img.shields.io/badge/document%20status-revising-orange?style=flat-square)

Here we will cover the first block, "*Download raw data*" in Figure \@ref(fig:diagram-overview-1)

```{r diagram-overview-1, fig.cap="Section of the overall workflow we will be covering.", echo=FALSE}
diagram_overview(1)
```
And your folder and file structure should look like:

```
LearnR3
├── data/
│   └── README.md
├── doc/
│   ├── README.md
│   └── lesson.Rmd
├── R/
│   ├── functions.R
│   └── README.md
├── .Rbuildignore
├── .gitignore
├── DESCRIPTION
├── LearnR3.Rproj
└── README.md
```

## Learning objectives

1. Learn about filesystems, relative and absolute paths, and how to make use 
of the fs package to navigate files in your project.
1. Learn where to store your raw data so that you can use
scripts as a record of what was done to process the data before analyzing it,
and why that's important.
1. Learn how to import data and do minor cleaning with the vroom package.
1. Learn about strategies and resources to use when encountering problems when
importing data.
1. Practice using Git version control as part of the workflow of data analysis.
    
## The MMASH dataset

For this course, we're going to use an openly licensed dataset on monitoring
sleep and activity (MMASH) [@Rossi2020]. We'll switch over to the [MMASH
website][mmash-website] and go over:

1. What is contained in the dataset by looking at the the Data Description.
We'll be making use of the Data Description for the code-along as well as the
exercises. **Take 5 min to quickly look over the Data Description and get more
familiar with it**.
1. The open license and the ability to re-use it. A small note: GDPR makes it
more strict on how to share and use personal data, but it *does not prohibit
sharing it or making it public*! GDPR and Open Data are not in conflict.

[mmash-website]: https://physionet.org/content/mmash/1.0.0/

> *Note*: Sometimes to PhysioNet website is slow. If that's the case,
use **[this alternative link](resources/mmash-page.html)** instead.

After we have looked over the MMASH website, we need to setup where we will store
and prepare the dataset for processing. Here we'll make use of the [usethis]
package to help setup things. usethis is an extremely useful package for managing
R projects and I highly recommend checking out how you can use it more in your
own work. For now, while in your `LearnR3` R Project, go to the Console pane in
RStudio and type out:

[usethis]: https://usethis.r-lib.org/

```{r create-data-raw, eval=FALSE}
usethis::use_data_raw("mmash")
```

What this function does is create a new folder called `data-raw/` and creates
an R script called `mmash.R` in the folder. This is where we will store the 
raw, original MMASH data that we'll get from the website. The R script should
have opened up for you, otherwise, go into the `data-raw/` folder and open up
the new `mmash.R` script.

The first thing we want do is create a new line at the top and type out:

```{r dr-script-add-here}
library(here)
```

The [here] package was described in the 
[Management of R Projects](https://r-cubed-v2.rostools.org/r-project-management.html)
of the introductory course. **Take 8 minutes to read the [section about
here][here-r-cubed] and read the next two paragraphs**. here makes it easy to
refer to other files in an R project.

R works based on the current *working directory*, which you can see on the top 
of the RStudio Console pane. When in an RStudio R Project, the working directory
is the folder where the `.Rproj` file is located. When you run scripts in with
`source()` sometimes the working directory will be where the R script is located.
So you can encounter problems with finding files. So instead, by using `here()`,
R knows to start searching for files from the `.Rproj`.

[here]: https://here.r-lib.org/
[here-r-cubed]: https://r-cubed-v2.rostools.org/r-project-management.html#packages-data-and-file-paths

Let's use an example. Below is the folder tree. If we open up RStudio with the
`LearnR3.Rproj` file and run code in the `data-raw/mmash.R`, R runs the commands
assuming everything starts in the `LearnR3/` folder. But! If we run the code in 
the `mmash.R` script by other ways (e.g. not with RStudio, not in an R Project,
or with `source()`), R runs everything assuming it starts in the `data-raw/`.
This can make things tricky. What `here()` does is tell R to first look for the
`.Rproj` file and then start looking for the file we actually want.

```text
LearnR3
├── data
│   └── README.md
├── data-raw
│   └── mmash.R
├── doc
│   ├── lesson.Rmd
│   └── README.md
├── R
│   └── README.md
├── .Rbuildignore
├── .gitignore
├── DESCRIPTION
├── LearnR3.Rproj
└── README.md
```

**Stop reading and we'll go back to coding together**. The first step we want
to take is to download the dataset. From this material, paste this code into the
`data-raw/mmash.R` script:

```{r mmash-link}
mmash_link <- "https://physionet.org/static/published-projects/mmash/multilevel-monitoring-of-activity-and-sleep-in-healthy-people-1.0.0.zip"
```

> *Note*: Sometimes the PhysioNet website is slow. If that's the case,
use `r3::mmash_data_link` instead of the link used above. In this case,
it will look like `mmash_link <- r3::mmash_data_link`.

Then we're going to write out the function `download.file()` to download and
save the zip dataset. We're going to save the zip file to
`data-raw/mmash-data.zip` with the `destfile` argument. This code should be 
written in the `data-raw/mmash.R` file.
Run these lines of code to download the dataset.

```{r download-data, eval=FALSE}
download.file(mmash_link, destfile = here("data-raw/mmash-data.zip"))
```

Because the original dataset is stored elsewhere, we don't need to keep it or 
save it to our Git history. So we'll add the zip file to the Git ignore list.
In the *Console*, type out and run this code. You only need to do this once.

```{r ignore-mmash-zip, eval=FALSE}
usethis::use_git_ignore("data-raw/mmash-data.zip")
```

This is a good time to save the changes we've made to the Git history.
Let's open the Git interface with either the Git icon at the top near the menu 
bar, or with `Ctrl-Alt-M`. When this opens up we'll click the checkbox beside
the `.gitignore` and `data-raw/mmash.R` files. Then we write a commit message
in the text box on the right, something like "Code to download data zip file".
Click the "Commit" button and close the Git interface.

Alright, let's start preparing the dataset. We'll open up the zip file and look
at what is inside (only instructor does this). Inside there is the license file,
another file to check if the download worked correctly (the SHA file),
and another zip of the dataset inside. Because we are starting with the original
raw `mmash-data.zip`, we should record exactly how we process the data set for
use. This also relates to the principal of *"keep your raw data raw"*, as in
don't edit or touch your raw data, let R or other programming language process 
it. This lets you have a history of what was done to the raw data.
During data collection, programs like Excel or Google Sheets are incredibly 
powerful. But after collection is done, don't make edits directly to the data
unless absolutely necessary.

A quick comment about whether you should save your *raw* data in `data-raw/`.
A general guideline is:

- *Do* store it to `data-raw/` if the data will only be used for the one project.
Use the `data-raw/` R script to be the record for how you processed your data
for final analysis work.
- *Don't* save it to `data-raw/` if: 1) there is a central dataset that multiple
people use for multiple projects; or 2) you got the data online. Instead, use
the `data-raw/` R script to be the record for which website you downloaded it
from or from which central location you extracted it from and how you processed
it.
- *Don't save it to a project-specific `data-raw/` folder if you will use the
raw data for multiple projects. Instead, create a central location for the data
*for yourself* so that you can point all other projects to it and use their
individual `data-raw/` R scripts as the record for how you processed the raw data.

Ok, let's start unzipping the zip files. In `data-raw/mmash.R`, continue writing 
below the `download.file()` function. We'll use the `unzip()` function
to unzip the dataset. The main argument for `unzip()` is the zip file,
and the other important one called `exdir` tells `unzip()` the folder we want to
extract the files to. The argument `junkpaths` is used here because we want 
everything extracted to the `data-raw/` folder (don't ask nor know why it's
called "junkpaths").

```{r unzip-main-file}
unzip(here("data-raw/mmash-data.zip"), 
      exdir = here("data-raw"),
      junkpaths = TRUE)
```

Notice the indentations and spacings of the code. Like writing any language,
code should follow a [style guide]. An easy way of following a style is by
selecting your code and using RStudio's builtin style fixer of either
`Ctrl-Shift-A` or "Code -> Reformat Code" menu item.
Ok, next, we want to extract the new `data-raw/MMASH.zip` file. Because we want to
keep the folder structure inside this zip file, we don't use `junkpaths`.

[style guide]: https://style.tidyverse.org

```{r unzip-data-file}
unzip(here("data-raw/MMASH.zip"),
      exdir = here("data-raw"))
```

Almost done! There are several files left over that we don't need, so we'll
also write in the script code to remove these files. We'll use the [fs] package,
which means filesystem, to work with files. First, we delete all the files
we originally extracted (`LICENSE.txt`, `SHA256SUMS.txt`, and `MMASH.zip`)
by using the `file_delete()` function. Then we'll rename the new folder
`data-raw/DataPaper/` to something more explicit like `data-raw/mmash/`
using the `file_move()` function. So the `data-raw/` folder will initially look
like:

[fs]: https://fs.r-lib.org/

```{r, eval=FALSE}
dir_tree("data-raw", recurse = 1)
```

```text
data-raw
├── LICENSE.txt
├── MMASH.zip
├── SHA256SUMS.txt
├── mmash
│   ├── user_1
│   ├── user_10
│   ├── ...
│   ├── user_8
│   └── user_9
├── mmash-data.zip
└── mmash.R
```

Then we add these lines of code to the `data-raw/mmash.R` script and run them:

```{r delete-rename-files}
library(fs)
file_delete(here(c("data-raw/MMASH.zip", 
                   "data-raw/SHA256SUMS.txt",
                   "data-raw/LICENSE.txt")))
file_move(here("data-raw/DataPaper"), here("data-raw/mmash"))
```

Afterward, the files and folders in `data-raw/` will look like:

```text
data-raw
├── mmash
│   ├── user_1
│   ├── user_10
│   ├── ...
│   ├── user_8
│   └── user_9
├── mmash-data.zip
└── mmash.R
```

Since we have an R script that downloads the data and processes it for us,
we don't need to have Git track it. So, in the *Console*, type out and run
this command:

```{r git-ignore-mmash, eval=FALSE}
usethis::use_git_ignore("data-raw/mmash/")
```

Now that we have everything prepared, let's add and commit the changes to the 
Git history.

## Importing in the raw data

While we'll eventually come back to the `data-raw/mmash.R` script,
for now we'll move over to the `doc/lesson.Rmd` file.
At the bottom of the file, create a header by typing out `## Importing raw data`.
Next, we'll make a new code chunk with `Ctrl-Alt-I` and call it `setup`.
Inside the code chunk we'll load the [vroom][vroom-website] package with
`library(vroom)` as well as `library(here)`. It should look like this:

[vroom-website]: https://vroom.r-lib.org/index.html

````markdown
`r ''````{r setup}
library(vroom)
library(here)
`r ''````
````

This is a specially named code chunk that tells R to run this code chunk first
whenever you first start running code in this R Markdown file. So it's here that
we will add `library()` functions when we want to load other packages.

**Take 5 minutes to read the next {{NUM}} paragraphs**.

What is [vroom]? It is a package designed to load in data, specifically
text-based data files such as CSV. In R there are several packages that you can
use to load in data and of different types of file formats. We won't cover
these other packages, but you can use this as a reference for when or if you
ever have to load other file types:

- [haven][haven]: For reading (also known as importing or loading) in SAS, SPSS, and
Stata files.
- [readxl][readxl]: For reading in Excel spreadsheets with `.xls` or `.xlsx` file
endings.
- [googlesheets4][googlesheets4]: For reading in Google Sheets from their cloud service.
- [readr][readr]: Standard package used to load in text-based data files like CSV. 
This package is included by default with tidyverse.
- [`utils::read.delim()`][read-delim]: This function comes from the core R
package utils and includes other functions like `utils::read.csv()`.
- [`data.table::fread()`][fread]: From the [data.table] package, used to load
in CSV files.

[data.table]: https://rdatatable.gitlab.io/data.table/
[haven]: https://haven.tidyverse.org/
[readxl]: https://readxl.tidyverse.org/
[googlesheets4]: https://googlesheets4.tidyverse.org/
[readr]: https://readr.tidyverse.org/
[read-delim]: https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/read.table
[fread]: https://rdatatable.gitlab.io/data.table/reference/fread.html

We're using the vroom package for largely one reason: It makes use of recent
improvements in R that allow data to be imported in very very quickly. 
Just how fast? The vroom website has a [benchmark page](https://vroom.r-lib.org/articles/benchmarks.html)
showing how fast it is. For many people, loading in the data can be one of
the most time-consuming
parts of starting an analysis. Hopefully by using this package, that time can be
reduced. 

The packages readr, vroom, haven, readxl, and googlesheets4 all are very
similar in how you use them and their documentation are almost identical. So
the skills you learn in this session with vroom can mostly be applied to 
these other packages.
And because readr (which the other packages are based on) has been around for
awhile, there is a large amount of support and help for using it.
If you're curious to learn more about vroom, check out the 
[website][vroom-website]

If your data is in CSV format, vroom is perfect. If not, there are other ways of
importing data which we won't cover. The CSV file format is a commonly used 
format for data because it is open, readable by any computer, and doesn't
depend on any special software to open (unlike for e.g. Excel spreadsheets).

```{r setup-02, include=FALSE}
library(vroom)
library(here)
```

```{r first-load-in-user-info}
user_1_info_file <- here("data-raw/mmash/user_1/user_info.csv")
user_1_info_data <- vroom(user_1_info_file)
```

You'll see the output mention using `spec()` to use in the argument `col_types`.
And that it has 5 columns, one called `...1`. If we look at the CSV file though,
we see that there are only four columns with names... but that technically 
there is a first empty column without a column header.
So, let's figure out what this message means. 
Let's go to the **Console** and type out:

```r
?vroom::spec
```

In the documentation, we see that it says:

> "extracts the full column specification from a tibble..."

Without seeing the output, it's not clear what "specification" means. So let's 
use `spec()` on the dataset variable. In the **Console** again:

```{r}
spec(user_1_info_data)
```

Ok, so from this we can see that a specification is what columns are imported
into R and what data type they are given. For instance, we can assume that
`col_double()` means numeric (double is how computers represent non-integer
numbers) and `col_character()` means a character data type.
Next, let's see what the message meant about `col_types`. Let's check out the
help documentation for `vroom()` by typing in the **Console**:

```r
?vroom::vroom
```

And if we scroll down to the explanation of `col_types`:

> "One of NULL, a cols() specification, or a string. See vignette("readr") for
more details."

It says to use a "cols() specification", which is likely the output of `spec()`.
So, let's copy and paste the output from `spec()` and paste it into the 
`col_types` argument of `vroom()`.

```{r vroom-with-error, error=TRUE}
user_1_info_data <- vroom(
    user_1_info_file,
    col_types = cols(
        ...1 = col_double(),
        Gender = col_character(),
        Weight = col_double(),
        Height = col_double(),
        Age = col_double(),
        .delim = ","
    )
)
```

Hmm. An error. Ok, if we look through the message, there's the part that says: 

> "The following named parsers don't match the column names: ...1"

We copy and pasted, so what's going on? If you recall, the `user_info.csv`
file has an empty column name. Looking at the [data dictionary][mmash-site]
it doesn't seem there is any reference to this column, so it seems it isn't 
important. Since we don't need it, let's just get rid of it when we load in 
the dataset. But how? Let's look at the help documentation again. Go to the 
**Console** and type out:

[mmash-site]: https://physionet.org/content/mmash/1.0.0/

```r
?vroom::vroom
```

Looking at the list of arguments, there is an argument called `col_select`
that sounds like we could use that to keep or drop columns. It says that it
is used similar to `dplyr::select()`, which normally is used with actual column 
names. Our column doesn't have a name, that's the problem. Next let's check the
Example section of the help. Scrolling down, you'll eventually see:

> `vroom(input_file, col_select = c(1, 3, 11))`

So, it takes numbers! With `dplyr::select()`, using the `-` before the column
name (or number) means to drop the column, so in this case, we could drop the
first column with `col_select = -1`!

```{r second-load-in-info-no-error}
user_1_info_data <- vroom(
    user_1_info_file,
    col_select = -1,
    col_types = cols(
        Gender = col_character(),
        Weight = col_double(),
        Height = col_double(),
        Age = col_double()
    )
)
```

Amazing! We did it `r emo::ji("grin")`

We can now look at the data:

```{r print-user-info-data}
user_1_info_data
```

Why might we use `spec()` and `col_types`? Depending on the size of the dataset,
it may take a long time to load everything, which may not be very efficient
if you only intend to use some parts of the dataset and not all of it.
And sometimes, `spec()` incorrectly guesses the column types, so using
`col_types = cols()` can fix those problems.

If you have a lot of columns in your dataset, then you can make use of
`col_select` or `cols_only()` to keep only the columns you want.
Check out vroom's website help documentation on 
[vroom's `col_select`](https://vroom.r-lib.org/reference/vroom.html) argument
or on [`cols_only()`](https://vroom.r-lib.org/reference/cols.html)
for more details on how to use those.

## Exercise: Import the saliva data {#ex-import-saliva}

Time: 15 min

Practice importing data files by doing the same process with the saliva data.

1. Create a new header at the bottom of the `doc/lesson.Rmd` file
and call it `## Exercise: Import the saliva data`.
1. Below the header, create a new code chunk with `Ctrl-Alt-I`.
1. Copy and paste the code template below into the new code chunk.
Begin replacing the `___` with the correct R functions or other information.
1. Look at the [data dictionary][mmash-site] and find out what the columns
mean and rename the column to be something more meaningful and
readable. TODO: Include this exercise? 
    - Pipe `%>%` the output from `vroom()` into the `rename()` function from the
    dplyr package to rename the column and use [`snake_case`][snake-case] when
    naming the new column. Read the `?dplyr::rename` help to know how to rename
    columns.
1. Once you have the code working, use the RStudio Git interface to add and 
commit the changes into the version history.

[snake-case]: https://simple.wikipedia.org/wiki/Snake_case

```{r solution-import-the-saliva-data, echo=FALSE, eval=FALSE}
library(dplyr)
user_1_saliva_file <- here("data-raw/mmash/user_1/saliva.csv")
user_1_saliva_data_prep <- vroom(user_1_saliva_file,
                                 col_select = -1)
spec(user_1_saliva_data_prep)

user_1_saliva_data <- vroom(
    user_1_saliva_file,
    col_select = -1,
    col_type = cols(
        SAMPLES = col_character(),
        `Cortisol NORM` = col_double(),
        `Melatonin NORM` = col_double()
    )
) %>%
    rename(... = SAMPLES,
           cortisol = `Cortisol NORM`,
           melatonin = `Melatonin NORM`)
```

## Importing larger datasets

Sometimes you may have a dataset that's just a bit too large. Sometimes vroom
may not have enough information to guess the data type of the column.
Or maybe there are hundreds or thousands of columns in your data
and you only want to import specific columns.
In these cases, we can do a trick: read in only first few lines of the dataset,
use `spec()` and paste in the output into the `col_type` argument,
and then keep only the columns you want to keep.

<!-- (use my UK Biobank analysis as example?) -->

Let's do this on the `RR.csv` file. 
We can see from the file size that it is bigger than most of the other files 
(~2Mb). So, we'll use this technique to decide what we want to keep etc. 
First, create a new header `## Import larger datasets` and a new code chunk below it
(with `Ctrl-Alt-I`). 

Do the same thing that we've been doing, but this time we are going to use the
argument `n_max`, which tells vroom how many rows to read into R. In this case,
let's read in 1000 to start. This dataset, like the others, has an empty column
that we will drop.

```{r import-some-of-data-trick}
user_1_rr_file <- here("data-raw/mmash/user_1/RR.csv")
user_1_rr_data_prep <- vroom(user_1_rr_file,
                             n_max = 1000,
                             col_select = -1)
spec(user_1_rr_data_prep)
```

Like with last time, copy and paste the output into a new use of `vroom()`.
Remove the `.delim` line and the `...1` line. Don't forget to also remove the
last `,` at the end! Make sure to remove the `n_max` argument, since we want to
import in the whole dataset.

```{r}
user_1_rr_data <- vroom(
    user_1_rr_file,
    col_select = -1,
    col_types = cols(
        ibi_s = col_double(),
        day = col_double(),
        # Converts to seconds
        time = col_time(format = "")
    )
) 
```

There's a new column type: `col_time()`. To see all the other types of column 
specifications, in the **Console** type out `col_` and then hit the Tab key. 
You'll see other types, like `col_date`, `col_factor`, and so on.
Right, what does the data look like?

```{r}
user_1_rr_data
```

To make sure everything is so far reproducible within the `lesson.Rmd` file,
we will "Knit" the R Markdown document to output an HTML file. Click the "Knit"
button at the top of the Source pane or by typing `Ctrl-Shift-K`. If it generates
an HTML document without problems, we know our code is at least starting to be
reproducible.

## Exercise: Import the Actigraph data

Time: 15 min

Practice some more. Do the same thing with the `Actigraph.csv` dataset as
we did for the `RR.csv`. But first:

1. Create a new header at the bottom of the `doc/lesson.Rmd` file
and call it `## Exercise: Import the Actigraph data`.
1. Below the header, create a new code chunk with `Ctrl-Alt-I`.

Use the same technique as we used for the `RR.csv` data 
and read in the `Actigraph.csv` file from `user_1/`.

1. Set the file path to the dataset with `here()`. 
1. Read in a max of 1000 rows for `n_max` and exclude the first column with
`col_select = -1`.
1. Use `spec()` to output the column specification and paste the results into
`col_types()`. Don't forget to remove the `...1 = col_skip()` and the 
`.delim = ","` line from the `cols()` function.
1. Read through the Data Description and rename the columns to be more readable
and explicit.
TODO: Change this last one?

```{r solution-import-the-actigraph-data, echo=FALSE, eval=FALSE}
# Use first 1000 or so lines to get spec
user_1_actigraph_file <- here("data-raw/mmash/user_1/Actigraph.csv")
user_1_actigraph_data_prep <- vroom(user_1_actigraph_file,
                                    n_max = 1000,
                                    col_select = -1)
spec(user_1_actigraph_data_prep)

user_1_actigraph_data <- vroom(
    user_1_actigraph_file,
    col_select = -1,
    col_types = cols(
        Axis1 = col_double(),
        Axis2 = col_double(),
        Axis3 = col_double(),
        Steps = col_double(),
        HR = col_double(),
        `Inclinometer Off` = col_double(),
        `Inclinometer Standing` = col_double(),
        `Inclinometer Sitting` = col_double(),
        `Inclinometer Lying` = col_double(),
        `Vector Magnitude` = col_double(),
        day = col_double(),
        time = col_time(format = "")
    )
)

# This will be taught later.
user_1_actigraph_data %>% rename_with(snakecase::to_snake_case)
```
