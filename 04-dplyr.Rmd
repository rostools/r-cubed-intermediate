# Processing datasets for cleaning {#dplyr-1}

![](https://img.shields.io/badge/document%20status-in%20progress-red?style=flat-square)

```{r, eval=TRUE, child="includes/preamble.Rmd"}
```

`r '<!-- '`

```{r setup-04, include=FALSE}
source("R/functions.R")
library(here)
library(fs)
library(dplyr)
library(stringr)
library(purrr)
library(vroom)
source_session("03-dry.Rmd")
knitr::opts_chunk$set(eval = FALSE)
```

Here we will continue making use of the "*Workflow*" block as we cover the third
block, "*Create final data*" in Figure \@ref(fig:diagram-overview-3).

```{r diagram-overview-3, fig.cap="Section of the overall workflow we will be covering.", echo=FALSE}
diagram_overview(3)
```

And your folder and file structure should look like:

```
LearnR3
├── data/
│   └── README.md
├── data-raw/
│   ├── mmash-data.zip
│   ├── mmash/
│   │  ├── user_1
│   │  ├── ...
│   │  └── user_22
│   └── mmash.R
├── doc/
│   ├── README.md
│   └── lesson.Rmd
├── R/
│   ├── functions.R
│   └── README.md
├── .Rbuildignore
├── .gitignore
├── DESCRIPTION
├── LearnR3.Rproj
└── README.md
```

## Processing character data

**Take 8 min to read this section before we quickly go over it together**.
When processing data, you likely will encounter and deal with cleaning up
character data. A wonderful package to use for working with character data is
called [stringr]. We'll use that in order to process the `file_path_id` 
so that we can get the user ID from it. First, let's go to the `setup`
code chunk and replace purrr with tidyverse and move `library(tidyverse)` to
the top of the `setup` code chunk.

The main driver behind the functions in stringr are [regular expressions] (or
[regex] for short). These expressions are powerful, very concise ways of finding
patterns in text. We've already used them in the `dir_ls()` function with the
`regexp` argument to find our data files.

To give an example, the regex `^.*[1-9][0-9]?` means "starting from the
beginning of the text, find any characters one or more times and stop at a
number from 0 to 9, include a possible other number from 0 to 9". So, using this
on a string like `"hi there, it's 30 degrees out"`, the regex would select `"hi
there, it's 30"`. If we break it down:

- `^` means start of the string.
- `.` means match any character once.
- `*` means match the previous character one or more times.
- `[]` means match whatever is in this brack once.
- `0-9` means match any number from 0 to 9.
- `?` means the previous thing may also be matched a second time.

Confused? Yea, regex does that to almost everyone, you aren't alone. While
regex can be very very powerful, it can also be incredibly difficult to write
and work with. We won't cover this anymore in this course, but two great resources
are the [R for Data Science regex section](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions),
the [stringr regex page](https://stringr.tidyverse.org/articles/regular-expressions.html),
as well as in the help doc `?regex`.
For now, we will only use it for the simplest purposes possible:
to extract `user_1` to `user_22` from the `file_path_id`.
**Please stop here and we'll go over this together**.

[stringr]: https://stringr.tidyverse.org/

`r details_for_instructors("
Make sure to re-inforce that while regex is incredibly complicated, there are some
basic things you can do with it that are quite powerful.

More or less, this section is to introduce the idea and concept of regex, but not
to really teach it since that is well beyond the scope of this course and this
time frame.
")`

## Exercise: Brainstorm a regex that will match for the user ID

Time: 5 min

Discuss with your neighbour what a potential regex might be to find
the ID in the `file_path_id`. Try not to look ahead `r emo::ji("wink")` since we
will use this regex later on. When the time is up, we'll share possible ideas.

## Exercise: What is the `%>%` pipe?

Time: 5 min

`r details_for_instructors("
Before starting this exercise, ask how many have used the pipe before.
If everyone has, then move on to the next section.
")`

We haven't used the `%>%` pipe from the [magrittr] package yet, but it is used
extensively in many R package, is the foundation of tidyverse packages,
and will eventually be incorporated into the next version of R (rather than
through magrittr). Because of this, we will make heavy use of it. To make sure
everyone is on the same page please do either:

[magrittr]: https://magrittr.tidyverse.org/

- If one of you or your pair doesn't know what the pipe is, take some time to 
talk about and explain it (if you know).
- If neither of the pair knows, please read 
[the section on it](https://r-cubed.rostools.org/wrangling.html#chaining-functions-with-the-pipe)
from the beginner course.

## Working with character columns

Now that we've talked about regex and pipes, let's start using them.
The first thing we'll do is work with the `user_info_df` to write
code that works, after which we will convert it into a function and move it
into the `R/functions.R` file.

We want to create a new column for the user ID, so we will use the `mutate()`
function from the [dplyr] package. We'll use the regex `user_[0-9][0-9]?` to match
for the user ID and we'll use the `str_extract()` function from the stringr
package. So, in your `doc/lesson.Rmd` file, create a new header called 
`## Using regex for user ID` at the bottom of the document, and create a new
code chunk below that.

[dplyr]: https://dplyr.tidyverse.org/

`r details_for_instructors("
Walk through writing this code, explain about how to use mutate, and about
the stringr function.
")`

```{r extract-user-id, eval=FALSE}
# Note: your file paths and data may look slightly different.
user_info_df %>% 
    mutate(user_id = str_extract(file_path_id, "user_[0-9][0-9]?"))
```

```{r extract-user-id-eval, echo=FALSE}
user_info_df %>% 
    mutate(file_path_id = gsub(".*\\/data-raw", "data-raw", file_path_id)) %>% 
    mutate(user_id = str_extract(file_path_id, "user_[0-9][0-9]?")) %>% 
    relocate(file_path_id, user_id)
```

Since we don't need to keep the `file_path_id`, let's drop it using `select()`
and `-`.

```{r drop-file-path-id}
user_info_df %>% 
    mutate(user_id = str_extract(file_path_id, "user_[0-9][0-9]?")) %>% 
    select(-file_path_id)
```

## Exercise: Convert this code into a function

Time: 10 min

We now have code that takes the data that has the `file_path_id` column
and extracts the user ID from it. While in the `doc/lesson.Rmd` file, convert
this code into a function, using the same process you've done previously.

- Call the new function `extract_user_id` and add one argument called
`imported_data`.
    - Don't forget to output the code into an object and adding `return()` at
    the end with the object inside it. 
    - Also include Roxygen documentation.
- After writing it and testing it, move the function into `R/functions.R`.
- Replace the code in the `doc/lesson.Rmd` file with the function name (e.g.
`extract_user_id(user_info_df)`), load everything with `load_all()`
(`Ctrl-Shift-L`), and run the new function.

*Tip*: If you don't know what package a function comes from when you
need to append the package when using `::`, you can find out what the package is
by using the help documentation `?functionname` (can also be done by pressing F1
when the cursor is over the function). The package name is at the very top left
corner, surrounded by `{ }`.

```{r solution-user-id-extract, echo=FALSE}
extract_user_id <- function(imported_data) {
    extracted_id <- imported_data %>% 
        dplyr::mutate(user_id = stringr::str_extract(file_path_id, 
                                                     "user_[0-9][0-9]?")) %>% 
        dplyr::select(-file_path_id)
    return(extracted_id)
}
```

Since we want this function to work on all data that we import, we should add it
to `import_multiple_files()`. After you've created the function, go to the
`import_multiple_files()` function in `R/functions.R` and use the `%>%` to
add it after using the `map_dfr()` function. The code should look something like:

```{r add-extract-user-to-import}
import_multiple_files <- function(file_pattern, import_function) {
    data_files <- fs::dir_ls(here::here("data-raw/mmash/"),
                             regexp = file_pattern,
                             recurse = TRUE)
    
    combined_data <- purrr::map_dfr(data_files, import_function,
                                    .id = "file_path_id") %>% 
        extract_user_id()
    return(combined_data)
}
```

Re-load the functions with `load_all()` (`Ctrl-Shift-L`). Then re-run these
pieces of code you wrote during Exercise \@ref(ex-function-import-all-data) to
update them based on the new code in the `import_multiple_files()` function.

```{r}
user_info_df <- import_multiple_files("user_info.csv", import_user_info)
saliva_df <- import_multiple_files("saliva.csv", import_saliva)
rr_df <- import_multiple_files("RR.csv", import_rr)
actigraph_df <- import_multiple_files("Actigraph.csv", import_actigraph)
```

## Join datasets together

The ability to join datasets together is a fundamental component of data processing
and transformation. In our case, we want to add the datasets together so we 
eventually have at least one main dataset to work with.

There are many ways to join datasets, the more common ones that are implemented
in the dplyr package are:
in the dplyr package are:

- `left_join(x, y)`: Join all rows and columns in `y` that match rows and
columns in `x`. *Columns* that exist in `y` but not `x` are joined to `x`.

    ```{r image-left-join, fig.cap="Left joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/left-join.svg"))
    ```
    
- `right_join(x, y)`: The opposite of `left_join()`. Join all rows and columns
in `x` that match rows and columns in `y`. *Columns* that exist in `x` but not `y`
are joined to `y`.

    ```{r image-right-join, fig.cap="Right joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/right-join.svg"))
    ```

- `full_join(x, y)`: Join all rows and columns in `y` that match rows and
columns in `x`. Columns *and* **rows** that exist in `y` but not `x` are joined
to `x`. This is probably the more commonly used one, as any missing values that
show up are probably important to look into.

    ```{r image-full-join, fig.cap="Full joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/full-join.svg"))
    ```

[dplyr-cheatsheet]: https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-transformation.pdf

In our case, we want to use `full_join()`, since we want all the data from both
datasets. This function takes two datasets and lets you indicate which column
to join by using the `by` argument. Here, both datasets have the column `user_id`
so we will join by them.

```{r}
full_join(user_info_df, saliva_df, by = "user_id")
```

We also eventually have other datasets to join together later on. Since
`full_join()` can only take two datasets at a time, do we then just keep 
using `full_join()` until all the other datasets are combined?
What if we get more data later on? Well, that's where more functional programming
comes in. Again, we have a simple goal: For a set of data frames, join them
all together. Here we use another functional programming concept called `reduce()`.
Like `map()`, which "maps" a function onto a set of items, `reduce()`
applies a function to each item of a vector or list, each time reducing the set
of items down until only one remains: the output. Let's use our simple function
`add_numbers()` from before and add up 1 to 5. Since `add_numbers()` only takes
two numbers, we have to give it two numbers at a time and repeat until we reach
5.

```{r add-numbers-function, echo=FALSE}
add_numbers <- function(num1, num2) {
    added <- num1 + num2
    return(added)
}
```

```{r}
# Add from 1 to 5
first <- add_numbers(1, 2)
second <- add_numbers(first, 3)
third <- add_numbers(second, 4)
add_numbers(third, 5)
```

Instead, we can use reduce to do the same thing:

```{r}
reduce(1:5, add_numbers)
```

This Figure \@ref(fig-reduce) visually shows what is happening within `reduce()`.

```{r image-reduce, fig.cap="A functional that iteratively uses a function on a set of items until only one output remains. Modified from the [RStudio purrr cheatsheet][purrr-cheatsheet].", out.width="75%", echo=FALSE}
knitr::include_graphics(here::here("images/reduce.svg"))
```
So, for our `full_join()`, if we put the datasets together as a list, we can
`reduce()` them down into one dataset with `full_join()`.

```{r}
combined_data <- reduce(list(user_info_df, saliva_df), full_join)
combined_data
```

## Renaming all columns

Functionals appear throughout R and are especially used frequently in the
tidyverse packages like dplyr. In order to continue processing the other datasets,
we need to get into these other functions.

```{r}
combined_data %>% 
    rename_with(snakecase::to_snake_case)
```


```{r}
import_rr <- function(file_path) {
    rr_data <- vroom::vroom(
        file_path,
        col_select = -1,
        col_types = vroom::cols(
            ibi_s = vroom::col_double(),
            day = vroom::col_double(),
            # Converts to seconds
            time = vroom::col_time(format = "")
        ),
        .name_repair = snakecase::to_snake_case
    ) 
    return(rr_data)
}
```

summarize across is also a functional in that you can give it many functions
to use on each of the columns you want the function to work on.

The across functional can also take a function when selecting the columns,
just like with the `select()` function
e.g. if you want to select only numeric columns you would use:

```{r}
user_info_df %>% 
    select(is.numeric)
```

Like wise with summarize, you can use across the same way:

```{r}
user_info_df %>% 
    summarize(across(is.numeric, mean))

user_info_df %>% 
    summarize(across(is.numeric, list(mean = mean, sd = sd)))
```

## Summarizing larger data to join

We use this concept to process the other longer datasets like `RR.csv` so we can
join them with the `user_info.csv` and other smaller datasets.

```{r}
import_multiple_files <- function(file_pattern, import_function) {
    data_files <- fs::dir_ls(here::here("data-raw/mmash/"),
                             regexp = file_pattern,
                             recurse = TRUE)
    
    combined_data <- purrr::map_dfr(data_files, import_function,
                                    .id = "file_path_id") %>% 
        extract_user_id()
    return(combined_data)
}
library(tidyverse)
all_rr_data <- import_multiple_files("RR.csv", import_rr)

all_rr_data %>% 
    group_by(user_id, day) %>% 
    summarize(across(ibi_s, list(mean = mean, sd = sd)))
```

We get a message:

```text
`summarise()` regrouping output by 'user_id' (override with `.groups` argument)
```

If we look into `?summarize` to the `.groups` argument,
we see that this argument is currently experimental. At the bottom there is a 
message about:

> In addition, a message informs you of that choice, unless the option
"dplyr.summarise.inform" is set to FALSE, or when summarise() is called from a
function in a package.

How would we do that? By using the `options()` function. So, go to the `setup`
code chunk and add this code to the top:

TODO: Make this true to show what it puts, since this is already in _common.R
```{r}
options(dplyr.summarize.inform = FALSE)
```



## Exercise: activity, calculate minutes from start and end.


## Exercise: Do same with Actigraph

- Look into documentation.
    - Rename columns to be more descriptive (e.g. what does `hr` mean? or `axis1`)
    by using the `rename()` from dplyr. Use the help `?rename` to figure out how 
    to use it if you don't know yet.
- Based on the documentation, which variables would you be most interested in
analyzing more?
    - Keep only those columns by using `select()`.
    - Decide which summary measure you think may be most interesting for you
    (e.g. `median()`, `sd()`, `mean()`, `max()`, `min()`, `var()`)
- Using `group_by()` of `user_id` and `day`, summarize the variables you chose

- group_by and summarize (FP)
- selecting


## Exercise

1. Look at the [data dictionary][mmash-site] and find out what the columns
mean and rename the column to be something more meaningful and
readable. TODO: Include this exercise? 
    - Pipe `%>%` the output from `vroom()` into the `rename()` function from the
    dplyr package to rename the column and use [`snake_case`][snake-case] when
    naming the new column. Read the `?dplyr::rename` help to know how to rename
    columns.

1. Read through the Data Description and rename the columns to be more readable
and explicit.
TODO: Change this last one?

## Wrangling data into final form

Save in data-raw script, source, and save data in data/

## Easily add parallel processing

**Take 8 minutes to read through this section and then move on to the last exercise**.
One major reason to get comfortable with and good at using purrr functions like 
`map()` is because it is relatively trivial to use parallel processing to speed
up your analysis. Packages like [furrr] (a combination of the [future] and [purrr] package)
have a series of functions starting with `future_` that can convert code using
`map()` into parallel processing code by switch to `future_map()`.
This extends to also `future_map_chr()`, `future_map_dfr()`, and so on.
What this does is creates multiple R sessions that run code simultaneously
and then eventually merges the results back into one R session.
But *note*, that parallel processing is good for some things and not for others.
If you code requires a specific sequence to run (1 relies on 2 which relies on 3),
parallel processing is not the right tool. It also doesn't always speed up tasks.
Creating the multiple R sessions and then merging them back takes some time,
so if your tasks is already pretty fast, using parallel processing might actually
make things slower.

furrr works by using the function `plan()` and setting a processing strategy.
There are really only two right now: `sequential` that you already use
and `multisession` that runs the parallel processing.
*Note*, the `plan()` function should **not** be put into a function. 
It should be included in an R script (like `data-raw/mmash.R`)
on its own. Here's an example of a 7 line R script:

[furrr]: https://davisvaughan.github.io/furrr/index.html
[future]: https://cran.r-project.org/package=future

```{r parallel-processing, eval=FALSE}
# Add this to the top:
library(furrr)
plan(multisession)

future_map(1:5, paste)

# Add this to end so it returns to normal
plan(sequential)
```

The ending should generally have `plan(sequential)` so you tell R to switch back 
to normal.

## Exercise: Add parallel processing to your raw data processing

Time: 10 min

Let's compare the difference between not using parallel processing and using it.
First:

1. Open the `data-raw/mmash.R` file.
1. Restart the R session `Ctrl-Shift-F10`.
1. Re-run the script with `source()` by using either the button "Source" at the
top right corner of the RStudio pane or with `Ctrl-Shift-S`.

After it finishes, then:

1. Add `library(furrr)` with the other `library()` functions at the top of 
`data-raw/mmash.R`.
1. Add `plan(multisession)` right below all the other library functions.
1. Go to the `import_multiple_df()` function in `R/functions.R` and replace
`purrr::map_dfr()` with `furrr::future_map_dfr`.
1. Go back to the `data-raw/mmash.R` script and add `plan(sequential)` to the very end.
1. Restart the R session with `Ctrl-Shift-F10`.
1. Re-run the code with `source()` using either the button "Source" at the top
right corner of the RStudio pane or with `Ctrl-Shift-S`. 

Do you notice a difference in speed compared to before?


